<?xml version="1.0"?>
<clickhouse>
    <!-- Memory and Buffer Settings for Maximum Performance -->
    <max_memory_usage>107374182400</max_memory_usage>  <!-- 100GB -->
    <max_memory_usage_for_user>107374182400</max_memory_usage_for_user>
    <max_server_memory_usage>107374182400</max_server_memory_usage>
    
    <!-- Query Performance Settings -->
    <max_threads>0</max_threads>  <!-- 0 = auto-detect CPU cores -->
    <max_query_size>1073741824</max_query_size>  <!-- 1GB query size limit -->
    <max_ast_depth>1000</max_ast_depth>
    <max_ast_elements>50000</max_ast_elements>
    
    <!-- Analytics Optimization -->
    <max_bytes_before_external_group_by>26843545600</max_bytes_before_external_group_by>  <!-- Disable external group by -->
    <max_bytes_before_external_sort>26843545600</max_bytes_before_external_sort>  <!-- Disable external sort -->
    <max_execution_time>300</max_execution_time>  <!-- 5 minute timeout for complex analytics -->
    
    <!-- Connection Settings -->
    <max_connections>1000</max_connections>
    <keep_alive_timeout>30</keep_alive_timeout>
    <max_concurrent_queries>500</max_concurrent_queries>
    
    <!-- HTTP Interface Settings -->
    <http_port>8123</http_port>
    <tcp_port>9000</tcp_port>
    <interserver_http_port>9009</interserver_http_port>
    
    <!-- Background Processing -->
    <background_pool_size>32</background_pool_size>
    <background_move_pool_size>8</background_move_pool_size>
    <background_fetches_pool_size>8</background_fetches_pool_size>
    <background_common_pool_size>8</background_common_pool_size>
    
    <!-- Storage and Compression -->
    <merge_tree>
        <max_bytes_to_merge_at_max_space_in_pool>161061273600</max_bytes_to_merge_at_max_space_in_pool>  <!-- 150GB -->
        <max_bytes_to_merge_at_min_space_in_pool>1048576</max_bytes_to_merge_at_min_space_in_pool>  <!-- 1MB -->
        <max_part_loading_threads>0</max_part_loading_threads>  <!-- auto-detect -->
        <max_part_removal_threads>0</max_part_removal_threads>  <!-- auto-detect -->
        <parts_to_delay_insert>1000</parts_to_delay_insert>
        <parts_to_throw_insert>3000</parts_to_throw_insert>
        <inactive_parts_to_delay_insert>0</inactive_parts_to_delay_insert>
        <inactive_parts_to_throw_insert>0</inactive_parts_to_throw_insert>
    </merge_tree>
    
    <!-- Compression Settings for Storage Efficiency -->
    <compression>
        <case>
            <min_part_size>10000000</min_part_size>  <!-- 10MB minimum for compression -->
            <min_part_size_ratio>0.01</min_part_size_ratio>
            <method>lz4</method>  <!-- Fast compression for frequent access -->
        </case>
        <case>
            <min_part_size>100000000</min_part_size>  <!-- 100MB minimum for high compression -->
            <min_part_size_ratio>0.01</min_part_size_ratio>
            <method>zstd</method>  <!-- High compression for archival data -->
            <level>3</level>  <!-- Balanced compression level -->
        </case>
    </compression>
    
    <!-- Query Cache for Analytics -->
    <query_cache>
        <max_size_in_bytes>1073741824</max_size_in_bytes>  <!-- 1GB cache -->
        <max_entries>10000</max_entries>
        <max_entry_size_in_bytes>104857600</max_entry_size_in_bytes>  <!-- 100MB per entry -->
        <max_entry_size_in_rows>1000000</max_entry_size_in_rows>  <!-- 1M rows per entry -->
    </query_cache>
    
    <!-- Logging and Monitoring -->
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
        <console>1</console>
    </logger>
    
    <!-- Query Log for Performance Monitoring -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_log>
    
    <!-- Part Log for Storage Monitoring -->
    <part_log>
        <database>system</database>
        <table>part_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </part_log>
</clickhouse> 